import numpy as np
import csv
import random
import math
import operator



def loadDataset(filename):
    #dataset = pd.read_csv(filename)
    l=1
    train_images = []
    train_labels = []
    test_images = []
    test_labels = []
    with open('C:\\Users\\saba\\Documents\\ML\\'+filename, 'r') as csvfile:
        lines = csv.reader(csvfile)
        dataset = list(lines)
        for x in range(len(dataset)):
            if(l!=1):
                train_labels.append(int(dataset[x][0]))
                results = list(map(int, dataset[x][1:len(dataset)-1]))
                train_images.append(results)
            else:
                print(dataset[x][0])
                print('inside else')
                l=l+1
    #print(train_labels)
    return train_images,train_labels


def predict_value(w,instance):
    sum = 0.0
    for i in range(len(w)):
        sum+=float(w[i])*int(instance[i])
    return (1/(1+np.exp(-sum)))
# weight using sigmoid function
def compute_weight(w,instance,actual,predicted):
    for i in range(len(w)):
        w[i]+=(int(actual)-float(predicted))* float(predicted)*(1-float(predicted))*int(instance[i])
    #print(w)
    return w


def one_training_pass(training_set,labels,w):
    #print('iteration')
    change_weight = [];
    error = 0
    # initialize weight
    for i in range(len(training_set[0])):
        change_weight.append(0)
    for i in range(len(training_set)):
        predicted = predict_value(w,training_set[i])
        if(predicted!= labels[i]):
            change_weight = compute_weight(change_weight,training_set[i],labels[i],predicted)
            error += predicted - labels[i]
    for j in range(len(w)):
        w[j]= w[j] + 0.1 * change_weight[j]
    return w,error

def train_perceptron_for_n(training_set,training_labels,n):

    new_labels = training_labels
    #initial w vector
    w = [0]
    for i in range(len(training_set[0])):
        w.append(random.random()/float(100))
    #append 1 for w0
    print('initial weights')
    print(w)
    x = np.array(training_set)
    y = []
    for i in range(len(training_set)):
        y.append([1])
    new_training_set = np.append(y, x, axis=1)
    for i in range(len(new_labels)):
        if(new_labels[i]==n):
            new_labels[i] = 1
        else:
            new_labels[i] = 0
    #print(new_labels)
    #print(new_labels)
    no_of_steps = 1
    w,error = one_training_pass(new_training_set,new_labels,w)
    #print(error)
    while (error > 0.0001 and no_of_steps!=100):
        w,error = one_training_pass(new_training_set,new_labels,w)
        no_of_steps+=1;
    #print(w)

    #print(new_training_set)
    #print(w)
    #print(len(w))
    return w



def getAccuracy(test_labels, predictions):
    correct = 0
    for x in range(len(predictions)):
        if test_labels[x] == predictions[x]:
            correct += 1
    print("total examples")
    print(len(predictions))
    print(correct)
    return (correct / float(len(predictions))) * 100.0


def main():
    train_images,train_labels = loadDataset("mnist_train1.csv")
    test_images,test_labels = loadDataset("mnist_test.csv")
    p0 = train_perceptron_for_n(train_images,train_labels,0)"""
    p0 = [-3.997104468336769e-09, 0.007946424338123792, 0.0024926910529353306, 0.0016520030020833031, 0.0029538545215296043, 0.009010100125930922, 0.009668205355647426, 0.0052098365969772695, 0.008426304926280166, 0.0013387073805419792, 0.001799896712624328, 0.0010903959004736408, 0.0025754877565888346, 0.005827767870870649, 0.0010725497360448677, 0.008358029474280415, 0.008473450971282244, 0.001118751113247277, 0.0012814192020098492, 0.009754151740748925, 0.007581753758924389, 0.0006899520842183615, 0.0018845924908368072, 0.007414889734012727, 0.009494079024284863, 0.0017194708502959498, 0.0007583558671023405, 0.009509063932041185, 0.0009696940314166059, 0.002868935041532886, 0.004757257814829915, 0.006682982149916166, 0.00924077330041121, 0.004676061939380663, 0.0036326016132334914, 0.008793662353215763, 0.0006555030899945269, 0.00788533818945758, 0.0011744897375108065, 0.004800844792287779, 0.007653600043721133, 0.003340531110102877, 0.007728920967771111, 0.0010153510135888165, 0.0074087206513197765, 0.008181205119571329, 0.009600301691591337, 0.009162588142079587, 0.0004365644206380348, 0.0004776161005797919, 0.002399928678147348, 0.004346995721366057, 0.0018651632853941213, 0.0005050450092597259, 0.0008087607775576211, 0.005994937571885165, 0.006073638261407544, 4.078701974773447e-05, 0.007426315660276942, 0.009072573131659297, 0.001084358643987582, 0.006724983971755538, 0.004339794140270764, 0.005111755293523873, 0.0008618828777113141, 0.00031935384697187266, 0.007235296405203646, 0.002511314437775196, 0.0013556069506942714, 0.002917660495851965, 0.009041543105321771, 0.007123740080461963, 0.0030078057430610785, 0.006991038283479456, 0.0005361390407803434, 0.0009925351533134662, 0.009426461469342017, 0.003005255175924256, 0.0026905089691457375, 0.009821130320686709, 0.0028850063474609, 0.00013611867063708093, 0.00708312075047966, 0.006545113222287477, 0.000903613345387927, 0.0046809772026618944, 0.0015323310685348324, 0.0005681578771560991, 0.0032567632307440564, 0.006180930259390875, 0.00017161606309631371, 0.008200358396321432, 0.009238497034683834, 0.00965453067088558, 0.0004848814238215593, 0.0006165310724924256, 0.008952223610414452, 0.0030689852170290967, 0.0022378402679751387, 0.006512789765396244, 0.0023600995373694363, 0.0028147532408292542, 0.003350693771284292, 0.0012678031588754157, 0.005153185346302703, 0.0044550449006262195, 0.0008520725123524454, 0.009344042320287585, 0.0039059733160374355, 0.002733514422123767, 0.0008765721735192566, 0.005997975511001649, 0.00017611681634655097, 0.0034509838995669807, 0.0017319801182384554, 0.006749556512128596, 0.0030979126716907048, 0.005098046549289431, 0.0009557183920194667, 0.008818708874381727, 0.007766693088966892, 0.00048100381892725807, 0.0055588350053494785, 0.007331695870281497, 0.001575959828941558, 0.008161456656608716, 0.008565418156273392, 0.009776328604258198, 0.0010549630954952557, 0.007404992299666236, 0.00463069893702465, 0.009759520155973452, 0.0063686482870663925, 0.005359934644395996, 0.008264502619937733, 0.0009301151224401316, 6.573318021694696e-05, 0.005112894286946781, 0.008246296082194298, 0.0022254277729074914, 0.00898943375465036, 0.009001604563578246, 0.004058298307827538, 0.009118713622602543, 0.0018783898433641277, 0.007808030389573937, 0.0043233671500038625, 0.0034840647890115404, 0.00712190109370912, 0.005135045399655777, 0.002085877980250026, 0.008811396581972638, 0.002699740269215087, 0.00882321844589689, 0.004221584319005924, 0.0006080289955023496, 0.007445938058181101, 0.0010227439483164763, 0.008964579725675171, 0.009549571063532585, 0.009953619892181837, 0.00988018240479428, 0.004791513094676991, 0.006272657559132473, 0.007382819778245644, 0.009626173198782638, 2.578039240671015e-05, 0.009836700047168856, 0.00514703857063307, 0.0006844289496876744, 0.00610527289891685, 0.0021497912346635584, 0.004267478340230312, 0.009866980854992807, 0.00604344422874757, 0.007661551447252748, 0.0066559523059558125, 0.0062396596458460084, 5.6594490878361106e-05, 4.253829005362975e-05, 0.008536823624636092, 0.00796957381749807, 0.004885122303935501, 0.005315034527312667, 0.007918385959688388, 0.0004952910994402951, 0.005300375233059566, 0.003264664121314201, 0.009381694924942362, 0.005076728297855049, 0.0038833558646082346, 0.005100728740839221, 0.009778993661234348, 0.003361331559024613, 0.007594519570296428, 0.0005680672599004488, 0.009230616065597717, 0.0011737254455217916, 0.003440549938485041, 0.0008766695598619889, 0.003904045510109769, 0.004427620128678616, 0.0024952641746396665, 1.5005261429135163e-05, 0.006539928459923715, 0.0006274132683461575, 0.005965729256027801, 0.009596139359184212, 0.0022273815312527966, 0.0075292851439290875, 0.0008769810747136842, 0.003959295323251198, 0.0024081869529915543, 0.008578247623117352, 0.008710442897665652, 0.0005963406149709527, 0.0010912771280120036, 0.006794341536170578, 0.008241784314527661, 0.00771540118107371, 0.009335351212056432, 0.00668971184363662, 0.000483342143674681, 0.0025090198343333016, 0.004021758402680774, 0.006493238288323631, 0.005023348189395827, 0.00792158600625812, 0.005422216969379949, 0.00766409050695633, 0.003845652478665149, 0.009312882708451417, 0.003495306039368563, 0.009984053592710991, 0.008720207112964923, 0.008974014574063936, 0.004537130431291007, 0.003936316672898367, 0.006461568722207737, 0.007578618755560563, 0.0023609085700172085, 0.009933863238083115, 0.005149353233428346, 0.0006952798425453377, 0.0002500946470619114, 0.00046758378605235465, 0.008244868267214132, 0.0013931571477635253, 0.0028400716002136396, 0.0017542285461817609, 0.003914898224591635, 0.007686358568317867, 0.0004695116941047506, 2.809391784481252e-05, 0.0047120234957393255, 0.0047431104581051944, 0.003502024810813834, 0.0006680168507938767, 0.009432941595832061, 0.0015261435598180407, 0.00024133422997174692, 0.008858465083237366, 0.009089057589756007, 0.0010357981637956482, 0.004363219524861035, 0.0071802856625207715, 0.0016233048929732608, 0.0009637013432777408, 0.0012234410531471026, 0.004869565050784203, 0.000286340489874044, 0.0027300451582850825, 0.006764312776965735, 0.006020295845354704, 0.008642209471347641, 0.007614830145512591, 0.005450083399505509, 0.005696595148076887, 0.0015221370825204716, 0.0016908139837306946, 0.007618872185046038, 0.0028893458623977285, 0.008787409981666296, 0.008640897702396951, 0.004022798526019056, 0.00470708811597237, 0.006184838053945124, 0.002614469489101521, 0.006575540553891244, 0.0026925000790214404, 0.009539822230815877, 0.008250320848005959, 0.0052398186322282925, 0.00489814850060897, 0.00969634055541929, 0.00582226825600965, 0.00042020963021347684, 0.00465201591638329, 0.00023928275993433167, 0.009752920452580232, 0.0010062645978433937, 0.001929025954035616, 0.00014056863956977584, 0.0039383960041817845, 0.008018888104756382, 0.005125443307020886, 0.0021762792275343266, 0.009409493197839479, 0.007680467548309276, 0.002676588853610955, 0.0014256954409789403, 0.005736226921088201, 0.004281536659011393, 0.0005883132194267704, 0.007380802293916018, 0.0023838131815803677, 0.003794020581494978, 0.008093980939234694, 0.0005014047939455745, 0.009854919857753573, 0.009180360975715222, 0.00043664386380499293, 0.004162211561858105, 0.005773180477473527, 0.001722022948040965, 0.00411255052052601, 0.0073455984039240895, 0.0052635196522517365, 0.003131018646984737, 0.0006146924633597151, 0.004625078056860344, 0.003433435297472075, 0.005308588687113566, 0.002216835698481222, 0.009868926975134662, 0.004567985193639327, 0.008020922751189218, 0.00381054742307119, 0.00281529665742251, 0.0006553517971515388, 0.005612944134131862, 0.009178336605819291, 0.009313933537052028, 0.0068163848513612246, 0.002917457532770463, 0.005466690881927485, 0.007285056331245492, 0.00509947553920702, 0.004807916005744999, 0.0024624632774488736, 0.006264293779766009, 0.00631536240766434, 0.0046001538950287855, 0.008448209125288935, 0.0071941374102946105, 0.0037908396875272186, 0.003512847243633108, 0.009902535751560426, 0.005319592879013034, 0.0006566226103352646, 0.005863411881732946, 0.006586060311629785, 0.008707914276834532, 0.005679221999615943, 0.004841729458322497, 0.00958060476218521, 0.0029480345573691057, 0.006774992253098985, 0.007794749155838574, 0.0005781128453406747, 0.0007253461714100873, 0.00949856314133762, 0.0017376785958521579, 0.0024627145496307124, 0.007511645705393466, 0.004006660245254621, 0.0042996537040364035, 0.004076060549036649, 0.0047194183784672016, 0.0016623297444905518, 0.0012242042575530759, 0.0002822356331108844, 0.0008287350227451729, 0.0015510705035249196, 0.008707107105076469, 0.008416566789124738, 0.0025793294830229086, 0.004318986860164309, 0.00838226476084876, 0.008520892131930259, 0.0012228913830722443, 0.006983823310692302, 0.003049912863005707, 0.009046768556142415, 0.007574962158966603, 0.0019383325608337586, 0.002291468963890175, 0.007462408078717495, 0.009927498641060753, 0.0011133717011135313, 0.003976411922227451, 0.007435792734385235, 0.008457825027326522, 0.0066592755122528435, 0.006428078996640342, 0.004812135304514179, 0.004582575537986775, 0.00842309464292209, 0.004979122686673955, 0.0016106280526970028, 0.004316292458069736, 0.00978970968755043, 0.0007280344528714644, 0.006232275244298987, 0.0047094769350968435, 0.001648385697320437, 0.0032203022881853362, 0.00287215978066244, 0.009969767123663742, 0.00576675478335809, 0.006229036451857968, 0.0018802594136600138, 0.0041721093659337, 0.008773933599176352, 0.002969713508112889, 0.00945525415484369, 0.0010547309279912553, 0.006590592956834821, 0.0006997724788299109, 0.006988736283055383, 0.00969335985904391, 0.006056552664173967, 0.006782552381700323, 0.008146632055265777, 0.00761036779370592, 0.005853626653627941, 0.0014844639176564923, 0.0037948015866461292, 0.004968690806131177, 0.009765782532418209, 0.005495213849403222, 0.0037804923483194474, 0.006464431232183101, 0.003721001795083947, 0.006729916464355278, 0.0014125888507983153, 0.008388367361952892, 0.004276626564385167, 0.00011875037426221092, 0.009863090239627852, 0.00276307022820262, 0.007081611317186042, 0.004913172825323596, 0.009036798022814825, 0.006662355191941638, 0.008413074958436198, 0.004128311311902073, 0.009703372662403691, 0.004837513400379804, 0.008036061833478752, 0.003977117450988874, 0.003061887845909133, 0.0019570674169591255, 0.003058012178148092, 0.003480380992030941, 0.008938349117348277, 0.009671909630599449, 0.0015952863594523325, 0.007090855017621063, 0.00498500506784501, 0.006093072187684648, 0.004246223216093247, 0.008765433939044638, 0.008050149207933942, 0.0026677260873870677, 0.0049672317243362974, 0.008790135464181647, 0.0020259876304255207, 0.004921296920912818, 0.005986508727268604, 0.0029047100108314894, 0.007762379332800595, 0.0031446781310968787, 0.004122590501500944, 0.009250789064838411, 0.002419936548237723, 0.0022414311235235095, 0.005491171937804515, 0.006067396985719513, 0.000556726874031804, 0.004690097747707443, 0.0027779350350357526, 0.00769145476566146, 0.0034423147970523596, 0.0028591134967492337, 0.005357161556662398, 0.0055835502027611695, 0.007013081094376079, 0.0021895896461422825, 0.0031539566129444884, 0.001050840484309199, 0.008730914725667924, 0.0038051944719830312, 0.004727493024001156, 0.006965665494993869, 0.00586779396613659, 0.0039450220861659505, 0.0015576915155785121, 0.009567818646493531, 0.002771585253479686, 0.003348677618515795, 0.008103749888835587, 0.002454375519124311, 0.007388925187780064, 0.008163415516694379, 0.0014747775201569913, 0.008329964768089377, 0.008836397127158876, 0.003681759184797307, 0.0015483577959842414, 0.0017632164107030416, 0.00048270382274643865, 0.0022789770699602864, 0.00925023703114621, 0.0005207886316500821, 0.0029247985435541966, 0.005283137246783292, 0.0036849834556044758, 0.006690044618210698, 0.005402288431609709, 0.0009618175503807091, 0.005113268077392989, 0.0026921203687381946, 0.002711122421323697, 0.005922806948082315, 0.009675986654711243, 0.00834222391632873, 0.009950195590105716, 0.007334853076330332, 0.0014563640949724122, 0.00892388131497456, 0.0025291040006648977, 0.0053196677391254, 0.0025412708940596605, 0.007347569051594214, 0.0017209479867430649, 0.0007533473125813163, 0.00829589277015468, 0.0096931021092412, 0.0005147782882946881, 0.007774548931197524, 0.009992013690578855, 0.002490198527522073, 0.002218918444944372, 0.0032449364324047968, 0.009384629847328485, 0.000985928202876968, 0.0043918567416203515, 0.00912730080153458, 0.00386052041547712, 0.001334802707542857, 0.009862751639518986, 0.0016496973340602483, 0.006421686652606279, 0.0058571487437206506, 0.008042479195299893, 0.009718977679971852, 0.0026189967201490473, 0.0022697317058113188, 0.008399532284174482, 0.005274545337132734, 0.005086807907294211, 0.0044774718881349885, 0.00570805804703934, 0.0058131918263231745, 0.008645862788036158, 0.008906944598094633, 0.009751231159558156, 0.0024190885088366115, 0.0030361536669518143, 0.006467144598032072, 0.006823997484680231, 0.007796905476037929, 0.006449353482789847, 0.007283890467774986, 0.001093750101775628, 0.004734231959707314, 0.008776475921040288, 0.0074985901632175335, 0.0006472773127711007, 0.007648857340971525, 0.0013148937540397598, 0.005171517238404002, 0.001537194355283117, 0.007178896104642547, 0.007983867334351264, 0.0008191764863619755, 0.0039042911390412636, 0.009091493028105626, 0.008288063391747532, 0.003946793150103903, 0.007272361533208542, 0.009258881945740495, 0.003555775780268783, 0.002243417583890339, 0.00023764435656892124, 0.0008436598156529329, 0.0019829202539005998, 0.0020143066891641736, 0.005103757004905649, 0.006667107002708506, 0.008156381894284968, 0.008460337425759787, 0.0010578786896271474, 0.004040890583809155, 0.004844577289576432, 0.0012059754350796403, 0.0016440539733611414, 0.0025969826209504155, 0.009113308719411108, 0.009277436819916886, 0.0022287343401122006, 0.0020360127732910794, 0.004843280074574265, 0.00977539207489902, 0.007085419361108857, 0.00534528291892488, 0.000781680650302607, 0.009898308399062758, 0.008179205210829389, 0.007515774457800437, 0.004124150727133791, 0.007113105788835293, 0.009516638751615345, 0.00821781278563683, 0.002821263696994094, 0.0018449145847932934, 0.004066857294840488, 0.008430037135193288, 0.008200492524661497, 0.006081003204111074, 0.008138821272067722, 0.007198072079014199, 0.000109928189614944, 0.004175878708218723, 0.005967933575851304, 0.005022171517376947, 0.001424242944429398, 0.009331197713958836, 0.001448816516664776, 0.005453627576141139, 0.0015642060240464372, 0.004842476443026913, 0.009754279587190093, 0.006182396067894879, 0.00882098857885361, 0.007707620943677016, 0.007960686021247887, 0.006824877910791658, 0.007242237823626544, 0.002619374537714721, 0.0033681502150836318, 0.007501966207368565, 0.0053222550261173476, 0.0033117243900177605, 0.002401366606014681, 0.0065362980423659366, 0.0011890279110062475, 0.005477519669064177, 0.004019638675099223, 0.005925288229882713, 0.006449890128773329, 0.006135999499444933, 0.002199958009973665, 0.0008056438064849292, 0.0001645340070092982, 0.009414767826001971, 0.008101074612325006, 0.0009404704386756801, 0.002768385745823285, 0.003260522232122959, 0.0026625921366065053, 0.006333413644806981, 0.008595801044737573, 0.006088419244357556, 0.005163681063859593, 0.0052303670774024105, 0.00011129625569402063, 0.00819810993056213, 0.008108202379440212, 0.005356999229230538, 0.007688546926072893, 0.006734520011515159, 0.008203575118231006, 0.006252585403718178, 0.009339638606561116, 9.654797462862131e-05, 0.0031495811649816384, 0.007075213414628132, 0.003249557764064491, 0.00820342058696257, 0.005956595845484652, 0.006825940056009806, 0.0013586387150778911, 0.006081156777696389, 0.009520741054499187, 0.00983164370040141, 0.002303111035223534, 0.0008512722841830412, 0.009196248508991008, 0.004408509466360884, 0.00814622838265846, 0.00648181032337793, 0.009491573830649084, 0.004299911663074806, 0.009996157171822036, 0.0021785235013596386, 0.000723164169809255, 0.006895640792140969, 0.0024424747502011924, 0.0065584962319162506, 0.00658331568777127, 0.005059567270911561, 0.004077940910871912, 0.0006851105121177281, 0.008052902425698871, 0.0029287973401670776, 0.008590741169097022, 0.005786139359494259, 0.0017135959703515958, 0.006432175529452725, 0.00019684570347960228, 0.007161709176563571, 0.0038328498821211076, 0.006406303824069323, 0.0038828018189116307, 0.009820233601553556, 0.008330407280087654, 0.004308388209676764, 0.005744073338104077, 0.007188743696330951, 0.00465662286078221, 0.0023315175904485975, 0.0026892558537080034, 0.006409597020542832, 0.0041466509085118, 0.004841067854689785, 0.007067621712463082, 0.003118073244304007, 0.0019198379836839973, 0.005175954232317682, 0.004989179449104339, 0.008226593279562038, 0.00507045257369686, 0.009519911310330026, 0.007173887407623455, 0.0014152090495941937, 0.004945963594835643, 0.0007393701671588404, 0.002420612746766805, 0.005452044019065739, 0.006617858961883114, 0.004109234493122733, 0.0027033571968272207, 0.005231996111817524, 0.008170394634014646, 0.009859290114080372, 0.0039616663995709735, 0.008090927152242638, 0.006876259694031305, 0.0036173507624956, 0.007171942683848598, 0.003443785241932962, 0.001841978282925707, 0.006089609829708985, 0.005873243426836216, 0.007730938026391975, 0.0019578700718369656, 0.008802635305101184, 0.00384746763841888, 0.002600296252007621, 0.00027439516769689807, 0.005699630996531052, 0.006832740488823599, 0.0065618006892082924]
    """
    print('......................................................................................')
    print(p0)
    #638 Misclassifications
    #p0=[-5754.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -26.0, -494.0, 1328.0, 3328.0, 1403.0, -226.0, 5919.0, 15673.0, 7589.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -104.0, 0.0, -17.0, -1258.0, -3247.0, -4311.0, -8047.0, -3754.0, -7247.0, -14106.0, -10400.0, -17650.0, -13767.0, -9831.0, -17252.0, -17034.0, -12097.0, -19006.0, -11490.0, -4991.0, -2995.0, -988.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -812.0, -762.0, -1139.0, -6267.0, -5988.0, 1949.0, -11972.0, -26428.0, -2054.0, -17263.0, 5738.0, -9627.0, -2995.0, -5766.0, -1872.0, -9440.0, -3588.0, -4989.0, -14011.0, -8352.0, 1062.0, -2027.0, -830.0, -188.0, 0.0, 0.0, 0.0, 0.0, -40.0, -98.0, -10951.0, -366.0, -5574.0, -5713.0, 5338.0, -8884.0, 1768.0, -1602.0, 4373.0, -7567.0, 7508.0, -10823.0, 5573.0, -3486.0, 3023.0, -1270.0, 136.0, -393.0, -1871.0, -9356.0, -4839.0, -450.0, 0.0, 0.0, 0.0, 0.0, -190.0, -871.0, -6408.0, 10797.0, -4624.0, -9731.0, 4425.0, 5825.0, -1535.0, -5069.0, 5678.0, -2353.0, 2656.0, -2676.0, -862.0, 1490.0, -5686.0, 334.0, -4252.0, -1362.0, -1575.0, -4328.0, -9092.0, -979.0, -820.0, 0.0, 0.0, -91.0, -502.0, -5231.0, -304.0, -4529.0, -989.0, 4272.0, -15300.0, 1611.0, -2112.0, -1927.0, 2345.0, 790.0, 767.0, -3156.0, 2203.0, -712.0, 3470.0, -4078.0, 6305.0, 4146.0, -3343.0, -11136.0, -25113.0, -9310.0, -2378.0, 0.0, -688.0, -166.0, -718.0, -4479.0, 1545.0, 706.0, 2226.0, -7680.0, 4300.0, 607.0, 193.0, -978.0, 3381.0, -1358.0, -1270.0, -5407.0, 4157.0, -1479.0, 4532.0, -6102.0, 688.0, -2580.0, 3625.0, -3017.0, -15695.0, -16573.0, -183.0, 3498.0, 0.0, 689.0, 2495.0, -4386.0, -10321.0, -4195.0, 4952.0, 1442.0, -424.0, 3164.0, -4289.0, 5361.0, -4823.0, 6042.0, -1818.0, 9417.0, -860.0, 3447.0, -3601.0, 6156.0, -915.0, 3463.0, -440.0, -12263.0, -18206.0, -8687.0, -324.0, 0.0, 0.0, 1553.0, 1736.0, -10384.0, 2937.0, 2951.0, -9633.0, 581.0, -38.0, 5633.0, -3343.0, -4046.0, 2917.0, -1715.0, -1752.0, 2165.0, 2179.0, -667.0, 8728.0, -1513.0, 756.0, -8639.0, 4080.0, -3622.0, -17755.0, -14771.0, -78.0, 0.0, 0.0, 5937.0, -3686.0, 4714.0, 4789.0, 52.0, 6293.0, -1856.0, 338.0, -2581.0, 1655.0, 1501.0, -3441.0, 2002.0, 2395.0, -98.0, 3643.0, 5201.0, -4448.0, 3671.0, -1139.0, 11313.0, 2811.0, 86.0, -20706.0, -19028.0, 0.0, 0.0, 1650.0, -3566.0, 943.0, -11766.0, -6569.0, 7369.0, 836.0, -198.0, -1321.0, -3308.0, 4083.0, -7217.0, 2176.0, -8400.0, -4691.0, -2236.0, 2454.0, -2285.0, 5847.0, -1405.0, 4848.0, -7004.0, 4633.0, 1699.0, -13482.0, -11592.0, 0.0, 0.0, 16.0, -1188.0, 1790.0, -13076.0, 3682.0, -7870.0, -684.0, -2982.0, 1926.0, -3058.0, -1690.0, 506.0, 2164.0, -3067.0, -10763.0, -3499.0, -796.0, -697.0, 1053.0, 5173.0, -463.0, 332.0, -2398.0, 5476.0, -5056.0, -6047.0, 0.0, 0.0, 63.0, 49.0, 5491.0, 4176.0, -2491.0, 13268.0, 810.0, 167.0, -1646.0, 2753.0, 1374.0, -376.0, -10600.0, 2412.0, -11471.0, -2263.0, -2690.0, 150.0, 1798.0, -4137.0, -2260.0, 5738.0, 3149.0, -7028.0, 10018.0, -3449.0, 0.0, 0.0, 0.0, -172.0, -7394.0, -48.0, 1849.0, -3204.0, 5669.0, 885.0, 5601.0, -5145.0, 736.0, 3314.0, -608.0, -17964.0, 948.0, -5306.0, 2592.0, 173.0, 1745.0, 4252.0, 1026.0, -2820.0, 7206.0, -4221.0, 1470.0, -12228.0, 0.0, 0.0, 0.0, -1211.0, -13496.0, 4930.0, 3145.0, -208.0, 6946.0, -862.0, 1793.0, 5254.0, -2687.0, 1654.0, -4068.0, -10492.0, -4007.0, -5943.0, -955.0, -2992.0, -4491.0, 1454.0, -1113.0, 7817.0, -5614.0, 14298.0, -5642.0, -8738.0, 0.0, 0.0, 0.0, -666.0, -12109.0, -5991.0, 6232.0, -3667.0, 3330.0, 2364.0, 3464.0, -640.0, 2736.0, -6259.0, -12082.0, -11198.0, 3299.0, -6214.0, 2040.0, -4552.0, 1498.0, 1229.0, 1780.0, -10153.0, 2193.0, -10727.0, -15540.0, -15077.0, 816.0, 0.0, 0.0, 0.0, -7558.0, 1589.0, 2480.0, 2026.0, 3082.0, -2544.0, -962.0, 651.0, -4036.0, 4643.0, -8390.0, -4825.0, -3816.0, -5556.0, 100.0, -2576.0, 1035.0, 2586.0, -5946.0, 11453.0, 318.0, 1967.0, -5499.0, -3133.0, 5162.0, 0.0, 0.0, -392.0, -7973.0, 972.0, -2485.0, -976.0, -3493.0, 4084.0, 1014.0, 6738.0, 1659.0, -4876.0, -9490.0, -4263.0, 801.0, -728.0, 1420.0, 525.0, -2254.0, -8889.0, 396.0, -4223.0, -4688.0, 4482.0, -7633.0, -6828.0, -4484.0, 0.0, 416.0, -481.0, -13440.0, -9039.0, -3010.0, 4699.0, -2133.0, 2722.0, -550.0, 2980.0, -5959.0, 11664.0, -10657.0, 1568.0, -5154.0, 168.0, -2531.0, 324.0, -3259.0, 5967.0, -1819.0, 3021.0, -2078.0, 1107.0, -12200.0, -1598.0, -256.0, 0.0, 1048.0, -906.0, -13122.0, 2197.0, 271.0, -4426.0, 6571.0, -1525.0, 15.0, 2801.0, 7139.0, -1257.0, 3147.0, -3938.0, 1836.0, -5725.0, -1356.0, 2352.0, -1967.0, 2715.0, 1645.0, -4078.0, -6430.0, -9246.0, -3354.0, 137.0, 0.0, 0.0, 0.0, -4670.0, 125.0, 4791.0, -823.0, 2980.0, -3005.0, -1372.0, 4331.0, 1747.0, 486.0, 4892.0, 259.0, -4735.0, -913.0, 3268.0, -10104.0, -364.0, -7183.0, -4438.0, -5188.0, 11201.0, -9946.0, -2947.0, -338.0, 5761.0, 0.0, 0.0, 0.0, -2923.0, 2452.0, -12241.0, -1761.0, 2000.0, 1269.0, 692.0, -1232.0, 527.0, 2155.0, -5796.0, 11490.0, -3203.0, -29.0, -239.0, 516.0, -7763.0, 3243.0, 6632.0, -3173.0, -159.0, -3590.0, -13346.0, 223.0, 712.0, 0.0, 0.0, 0.0, -376.0, -945.0, -14589.0, 1065.0, 3416.0, -3581.0, -2238.0, 1353.0, 3848.0, -2225.0, -34.0, -457.0, 2817.0, -2999.0, -4966.0, 8488.0, -8588.0, 799.0, -17110.0, -3007.0, 2766.0, -1748.0, -13619.0, 270.0, 0.0, 0.0, 0.0, 0.0, -1085.0, -1670.0, 1961.0, -1219.0, -2591.0, -1708.0, -1423.0, 4226.0, -4575.0, -3033.0, 4307.0, -4110.0, 2240.0, 1406.0, -9490.0, 3780.0, 781.0, -10329.0, -6357.0, -6538.0, -9305.0, -7373.0, -340.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -2512.0, -6587.0, -12540.0, -15691.0, -20639.0, -5856.0, -21110.0, -29.0, -13924.0, -17823.0, -9091.0, -18244.0, -6944.0, -20540.0, -24922.0, -11418.0, -15667.0, -13303.0, -7947.0, -5311.0, -495.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -1068.0, -1492.0, -2616.0, -5313.0, -9429.0, -8518.0, -15063.0, -9232.0, -14586.0, -14235.0, -16395.0, -13118.0, -9134.0, -7466.0, -983.0, -8300.0, -10617.0, -5729.0, -1515.0, -28.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -114.0, -490.0, -316.0, 0.0, -8.0, -956.0, -3507.0, -74.0, -1411.0, -2794.0, -1878.0, -997.0, -585.0, -732.0, -206.0, -253.0, -1960.0, -6096.0, -1392.0, 0.0, 0.0, 0.0, 0.0]
    p1 = train_perceptron_for_n(train_images,train_labels,1)
    print('......................................................................................')
    print(p1)
    p2 = train_perceptron_for_n(train_images,train_labels,2)
    print('......................................................................................')
    print(p2)
    p3 = train_perceptron_for_n(train_images,train_labels,3)
    print('......................................................................................')
    print(p3)
    p4 = train_perceptron_for_n(train_images,train_labels,4)
    print('......................................................................................')
    print(p4)
    p5 = train_perceptron_for_n(train_images,train_labels,5)
    print('......................................................................................')
    print(p5)
    p6 = train_perceptron_for_n(train_images,train_labels,6)
    print('......................................................................................')
    print(p6)
    p7 = train_perceptron_for_n(train_images,train_labels,7)
    print('......................................................................................')
    print(p7)
    p8 = train_perceptron_for_n(train_images,train_labels,8)
    print('......................................................................................')
    print(p8)
    p9 = train_perceptron_for_n(train_images,train_labels,9)
    print('......................................................................................')
    print(p9)
    x = np.array(test_images)
    y = []
    for i in range(len(test_images)):
        y.append([1])
    new_testing_data = np.append(y, x, axis=1)
    predictions = []
    for i in range(len(new_testing_data)):
        result = []
        result.append(predict_value(p0,new_testing_data[i]))
        result.append(predict_value(p1,new_testing_data[i]))
        result.append(predict_value(p2,new_testing_data[i]))
        result.append(predict_value(p3,new_testing_data[i]))
        result.append(predict_value(p4,new_testing_data[i]))
        result.append(predict_value(p5,new_testing_data[i]))
        result.append(predict_value(p6,new_testing_data[i]))
        result.append(predict_value(p7,new_testing_data[i]))
        result.append(predict_value(p8,new_testing_data[i]))
        result.append(predict_value(p9,new_testing_data[i]))
        predictions.append(np.argmax(result))
    print("predicted value "+prediction[i]+"actual value "+test_labels[i])
    accuracy = getAccuracy(test_labels, predictions)
    print("accuracy "+accuracy)

main()